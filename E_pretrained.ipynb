{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Vc4zjy6ASw7U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c28f66c-fdd1-4e39-89fa-791922604f35"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AutoModelForAudioClassification - pretrained"
      ],
      "metadata": {
        "id": "zwAi_R0jwd_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from glob import glob\n",
        "from typing import List, Optional, Union, Dict\n",
        "\n",
        "import tqdm\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import functional as F\n",
        "from transformers import (\n",
        "    AutoFeatureExtractor,\n",
        "    AutoModelForAudioClassification,\n",
        "    Wav2Vec2Processor\n",
        ")\n"
      ],
      "metadata": {
        "id": "-FJWDbn6whiF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqFW7tR7Grsj"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define paths\n",
        "train_zip_path = '/content/drive/MyDrive/YoungCon/train.zip'\n",
        "test_zip_path = '/content/drive/MyDrive/YoungCon/test.zip'\n",
        "train_extract_path = '/content/train'\n",
        "test_extract_path = '/content/test'\n",
        "labels_folder = '/content/labels'\n",
        "\n",
        "# Unpack train.zip\n",
        "with zipfile.ZipFile(train_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(train_extract_path)\n",
        "\n",
        "# Unpack test.zip\n",
        "with zipfile.ZipFile(test_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(test_extract_path)\n",
        "\n",
        "# Move targets.tsv to labels folder\n",
        "if not os.path.exists(labels_folder):\n",
        "    os.makedirs(labels_folder)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.rename(os.path.join('/content/train/train/targets.tsv'), os.path.join(labels_folder, 'targets.tsv'))"
      ],
      "metadata": {
        "id": "XJPIoYcIKFQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def check_files_in_targets(train_folder, targets_file):\n",
        "    \"\"\"\n",
        "    Check if every file in the train folder is listed in the targets file.\n",
        "    Add \".wav\" extension to every record in the targets file.\n",
        "\n",
        "    Args:\n",
        "        train_folder (str): Path to the train folder containing audio files.\n",
        "        targets_file (str): Path to the targets.tsv file.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if all files in the train folder are listed in the targets file, False otherwise.\n",
        "        list: List of files in the train folder not listed in the targets file.\n",
        "    \"\"\"\n",
        "    # Read the targets file\n",
        "    targets_df = pd.read_csv(targets_file, sep='\\t')\n",
        "\n",
        "    # Add \".wav\" extension to each record in the targets file\n",
        "    targets_df.iloc[:, 0] = targets_df.iloc[:, 0].apply(lambda x: f\"{x}.wav\")\n",
        "\n",
        "    # Get the list of files in the train folder\n",
        "    train_files = [f for f in os.listdir(train_folder) if os.path.isfile(os.path.join(train_folder, f))]\n",
        "\n",
        "    # Get the list of files from the targets file\n",
        "    target_files = targets_df.iloc[:, 0].tolist()\n",
        "\n",
        "    # Check for files in the train folder not in the targets file\n",
        "    missing_files = [f for f in train_files if f not in target_files]\n",
        "\n",
        "    # Return the result\n",
        "    if missing_files:\n",
        "        return False, missing_files\n",
        "    else:\n",
        "        return True, []\n",
        "\n",
        "# Example usage\n",
        "train_folder = '/content/train/train'\n",
        "targets_file = '/content/labels/targets.tsv'\n",
        "\n",
        "all_files_present, missing_files = check_files_in_targets(train_folder, targets_file)\n",
        "if all_files_present:\n",
        "    print(\"All files in the train folder are listed in the targets file.\")\n",
        "else:\n",
        "    print(\"The following files in the train folder are not listed in the targets file:\")\n",
        "    for file in missing_files:\n",
        "        print(file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4F_eaEM-MRnL",
        "outputId": "2f304538-c2be-4e80-df78-b64d729c589a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following files in the train folder are not listed in the targets file:\n",
            "5d1f7e43366513a1d0a6ec5640c3dc24.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio\n",
        "import torch\n",
        "\n",
        "# Function to delete a specific file\n",
        "def delete_file(file_path):\n",
        "    if os.path.exists(file_path):\n",
        "        os.remove(file_path)\n",
        "        print(f\"Deleted file: {file_path}\")\n",
        "    else:\n",
        "        print(f\"File {file_path} not found.\")\n",
        "\n",
        "# Delete the specified file from the train folder\n",
        "train_audio_dir = '/content/train/train'\n",
        "file_to_delete = '5d1f7e43366513a1d0a6ec5640c3dc24.wav'\n",
        "delete_file(os.path.join(train_audio_dir, file_to_delete))\n",
        "\n",
        "# Load and modify targets\n",
        "targets_path = '/content/labels/targets.tsv'\n",
        "labels_df = pd.read_csv(targets_path, sep='\\t')\n",
        "labels_df.iloc[:, 0] = labels_df.iloc[:, 0].apply(lambda x: f\"{x}.wav\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dlPNea4NZDy",
        "outputId": "c62a6969-2bfb-4df2-9924-6a1c8d27e92e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted file: /content/train/train/5d1f7e43366513a1d0a6ec5640c3dc24.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset: List,\n",
        "        basedir: Optional[str] = None,\n",
        "        sampling_rate: int = 16000,\n",
        "        max_audio_len: int = 5,\n",
        "    ):\n",
        "        self.dataset = dataset\n",
        "        self.basedir = basedir\n",
        "\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.max_audio_len = max_audio_len\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the length of the dataset\n",
        "        \"\"\"\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.basedir is None:\n",
        "            filepath = self.dataset[index]\n",
        "        else:\n",
        "            filepath = os.path.join(self.basedir, self.dataset[index])\n",
        "\n",
        "        speech_array, sr = torchaudio.load(filepath)\n",
        "\n",
        "        if speech_array.shape[0] > 1:\n",
        "            speech_array = torch.mean(speech_array, dim=0, keepdim=True)\n",
        "\n",
        "        if sr != self.sampling_rate:\n",
        "            transform = torchaudio.transforms.Resample(sr, self.sampling_rate)\n",
        "            speech_array = transform(speech_array)\n",
        "            sr = self.sampling_rate\n",
        "\n",
        "        len_audio = speech_array.shape[1]\n",
        "\n",
        "        # Pad or truncate the audio to match the desired length\n",
        "        if len_audio < self.max_audio_len * self.sampling_rate:\n",
        "            # Pad the audio if it's shorter than the desired length\n",
        "            padding = torch.zeros(1, self.max_audio_len * self.sampling_rate - len_audio)\n",
        "            speech_array = torch.cat([speech_array, padding], dim=1)\n",
        "        else:\n",
        "            # Truncate the audio if it's longer than the desired length\n",
        "            speech_array = speech_array[:, :self.max_audio_len * self.sampling_rate]\n",
        "\n",
        "        speech_array = speech_array.squeeze().numpy()\n",
        "\n",
        "        return {\"input_values\": speech_array, \"attention_mask\": None}\n",
        "\n",
        "\n",
        "class CollateFunc:\n",
        "    def __init__(\n",
        "        self,\n",
        "        processor: Wav2Vec2Processor,\n",
        "        padding: Union[bool, str] = True,\n",
        "        pad_to_multiple_of: Optional[int] = None,\n",
        "        return_attention_mask: bool = True,\n",
        "        sampling_rate: int = 16000,\n",
        "        max_length: Optional[int] = None,\n",
        "    ):\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.processor = processor\n",
        "        self.padding = padding\n",
        "        self.pad_to_multiple_of = pad_to_multiple_of\n",
        "        self.return_attention_mask = return_attention_mask\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __call__(self, batch: List[Dict[str, np.ndarray]]):\n",
        "        # Extract input_values from the batch\n",
        "        input_values = [item[\"input_values\"] for item in batch]\n",
        "\n",
        "        batch = self.processor(\n",
        "            input_values,\n",
        "            sampling_rate=self.sampling_rate,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_attention_mask=self.return_attention_mask\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_values\": batch.input_values,\n",
        "            \"attention_mask\": batch.attention_mask if self.return_attention_mask else None\n",
        "        }"
      ],
      "metadata": {
        "id": "UZE4EeSHwfc6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Optional, Dict, Union\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset: List,\n",
        "        basedir: Optional[str] = None,\n",
        "        sampling_rate: int = 16000,\n",
        "        max_audio_len: int = 5,\n",
        "    ):\n",
        "        self.dataset = dataset\n",
        "        self.basedir = basedir\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.max_audio_len = max_audio_len\n",
        "        self.audio_file_names = [os.path.basename(f) for f in dataset]  # Store file names\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the length of the dataset\n",
        "        \"\"\"\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.basedir is None:\n",
        "            filepath = self.dataset[index]\n",
        "        else:\n",
        "            filepath = os.path.join(self.basedir, self.dataset[index])\n",
        "\n",
        "        speech_array, sr = torchaudio.load(filepath)\n",
        "\n",
        "        if speech_array.shape[0] > 1:\n",
        "            speech_array = torch.mean(speech_array, dim=0, keepdim=True)\n",
        "\n",
        "        if sr != self.sampling_rate:\n",
        "            transform = torchaudio.transforms.Resample(sr, self.sampling_rate)\n",
        "            speech_array = transform(speech_array)\n",
        "            sr = self.sampling_rate\n",
        "\n",
        "        len_audio = speech_array.shape[1]\n",
        "\n",
        "        # Pad or truncate the audio to match the desired length\n",
        "        if len_audio < self.max_audio_len * self.sampling_rate:\n",
        "            # Pad the audio if it's shorter than the desired length\n",
        "            padding = torch.zeros(1, self.max_audio_len * self.sampling_rate - len_audio)\n",
        "            speech_array = torch.cat([speech_array, padding], dim=1)\n",
        "        else:\n",
        "            # Truncate the audio if it's longer than the desired length\n",
        "            speech_array = speech_array[:, :self.max_audio_len * self.sampling_rate]\n",
        "\n",
        "        speech_array = speech_array.squeeze().numpy()\n",
        "        audio_file_name = self.audio_file_names[index]\n",
        "\n",
        "        return {\"input_values\": speech_array, \"attention_mask\": None, \"audio_file_name\": audio_file_name}\n"
      ],
      "metadata": {
        "id": "5zZw5fhQV5ny"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import tqdm\n",
        "\n",
        "def predict(test_dataloader, model, device: torch.device):\n",
        "    \"\"\"\n",
        "    Predict the class of the audio and get filenames of uncertain predictions\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    uncertain_filenames = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm.tqdm(test_dataloader):\n",
        "            #print(batch['audio_file_name'])\n",
        "            input_values, attention_mask = batch['input_values'].to(device), batch['attention_mask'].to(device)\n",
        "            logits = model(input_values, attention_mask=attention_mask).logits\n",
        "            scores = F.softmax(logits, dim=-1)\n",
        "            max_scores, pred = torch.max(scores, dim=1)\n",
        "\n",
        "            # Track filenames with predictions in the confidence range (0.45, 0.55)\n",
        "            for i in range(len(pred)):\n",
        "                if 0.01 < max_scores[i].item() < 0.99:\n",
        "                    print(max_scores[i].item(), batch['audio_file_name'][i])\n",
        "                    uncertain_filenames.append(batch['audio_file_name'][i])\n",
        "\n",
        "            preds.extend(pred.cpu().detach().numpy())\n",
        "\n",
        "    return preds, uncertain_filenames"
      ],
      "metadata": {
        "id": "hL3a1PleV8kR"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_audio_file_paths(directory, extensions=['.wav', '.mp3', '.flac']):\n",
        "    audio_files = []\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if any(file.lower().endswith(ext) for ext in extensions):\n",
        "                audio_files.append(os.path.abspath(os.path.join(root, file)))\n",
        "    return audio_files"
      ],
      "metadata": {
        "id": "XcTifOVPWEkv"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the directory containing the audio files\n",
        "model_name_or_path = \"/content\"\n",
        "directory = '/content/train/train'\n",
        "audio_paths = get_audio_file_paths(directory)[:1000] # Must be a list with absolute paths of the audios that will be used in inference\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "label2id = {\n",
        "    \"female\": 1,\n",
        "    \"male\": 0\n",
        "}\n",
        "\n",
        "id2label = {\n",
        "    1: \"female\",\n",
        "    0: \"male\"\n",
        "}\n",
        "\n",
        "num_labels = 2"
      ],
      "metadata": {
        "id": "Gnjmbchu2IFj"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2Processor\n",
        "\n",
        "class CollateFunc:\n",
        "    def __init__(\n",
        "        self,\n",
        "        processor: Wav2Vec2Processor,\n",
        "        padding: Union[bool, str] = True,\n",
        "        pad_to_multiple_of: Optional[int] = None,\n",
        "        return_attention_mask: bool = True,\n",
        "        sampling_rate: int = 16000,\n",
        "        max_length: Optional[int] = None,\n",
        "    ):\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.processor = processor\n",
        "        self.padding = padding\n",
        "        self.pad_to_multiple_of = pad_to_multiple_of\n",
        "        self.return_attention_mask = return_attention_mask\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __call__(self, batch: List[Dict[str, np.ndarray]]):\n",
        "        # Extract input_values and audio_file_names from the batch\n",
        "        input_values = [item[\"input_values\"] for item in batch]\n",
        "        audio_file_names = [item[\"audio_file_name\"] for item in batch]\n",
        "\n",
        "        batch = self.processor(\n",
        "            input_values,\n",
        "            sampling_rate=self.sampling_rate,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_attention_mask=self.return_attention_mask\n",
        "        )\n",
        "\n",
        "        # Return the batch along with audio file names\n",
        "        return {\n",
        "            \"input_values\": batch.input_values,\n",
        "            \"attention_mask\": batch.attention_mask if self.return_attention_mask else None,\n",
        "            \"audio_file_name\": audio_file_names\n",
        "        }\n"
      ],
      "metadata": {
        "id": "iOyLTON_XQVt"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = CustomDataset(audio_paths, max_audio_len=5)  # for 5-second audio\n",
        "\n",
        "data_collator = CollateFunc(\n",
        "    processor=feature_extractor,\n",
        "    padding=True,\n",
        "    sampling_rate=16000,\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")"
      ],
      "metadata": {
        "id": "1N0lWHPq2pnz"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_labels = 2\n",
        "\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name_or_path)\n",
        "model = AutoModelForAudioClassification.from_pretrained(\n",
        "    pretrained_model_name_or_path=model_name_or_path,\n",
        "    use_safetensors=True,\n",
        "    num_labels=num_labels,\n",
        "    label2id=label2id,\n",
        "    id2label=id2label,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u86-U-d12WMY",
        "outputId": "fd1d5b60-b29d-4861-adc0-16ded36a0ed7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at /content were not used when initializing Wav2Vec2ForSequenceClassification: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at /content and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = predict(test_dataloader=test_dataloader, model=model, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L47i0cOl2LJf",
        "outputId": "c6166862-742b-499f-914e-085965743653"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 2/63 [00:09<04:08,  4.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5536decc9056cd290e16bc3773c24f98.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 8/63 [00:15<01:04,  1.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9d8d8f1c55253f8b756c6cfe6880a90d.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 16/63 [00:23<00:47,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6e0e2fdb0bc1fb3d8b38b5eb71808c8a.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|███▎      | 21/63 [00:28<00:43,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3c79898446b0cb965122339c17ea6e1c.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 42/63 [00:50<00:21,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "874be2c6eb2123c77d338eb82638f88a.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 79%|███████▉  | 50/63 [00:58<00:12,  1.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ce82fece3130d24a2717e8832fd36f25.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▏| 58/63 [01:06<00:04,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6fce7d9b657c09f7cc8b7fc99eebd93d.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 63/63 [01:10<00:00,  1.12s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.savetxt(\"preds_train.npy\", np.array(preds))"
      ],
      "metadata": {
        "id": "cnqt1nb55_f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Path to the targets.tsv file\n",
        "targets_file_path = \"/content/labels/targets.tsv\"\n",
        "\n",
        "# Sample paths to audio files and predictions from the model\n",
        "#audio_paths = [\"/path/to/audio1.wav\", \"/path/to/audio2.wav\"]  # Example list of audio paths\n",
        "#preds = [1, 0]  # Example predictions, 1 for female, 0 for male\n",
        "\n",
        "# Load the targets.tsv file into a pandas DataFrame\n",
        "targets_df = pd.read_csv(targets_file_path, sep='\\t', header=None, names=['audio_id', 'true_label'])\n",
        "\n",
        "# Append \".wav\" to audio_id to match the audio file paths\n",
        "targets_df['audio_id'] = targets_df['audio_id'] + '.wav'\n",
        "\n",
        "# Create a dictionary from audio_id to true_label\n",
        "true_labels_dict = dict(zip(targets_df['audio_id'], targets_df['true_label']))\n",
        "\n",
        "# Initialize counters for correct and total predictions\n",
        "correct_predictions = 0\n",
        "total_predictions = len(audio_paths)\n",
        "\n",
        "# Prepare data for writing to a new file\n",
        "output_data = []\n",
        "\n",
        "# Compare predictions with true labels and prepare the output data\n",
        "for audio_path, pred in zip(audio_paths, preds):\n",
        "    audio_id = os.path.basename(audio_path)  # Get the audio_id from the file path\n",
        "    true_label = true_labels_dict.get(audio_id, None)\n",
        "    if pred==0:\n",
        "        pred=1\n",
        "    else:\n",
        "        pred=0\n",
        "    if true_label is not None:\n",
        "        if pred == true_label:\n",
        "            correct_predictions += 1\n",
        "        else:\n",
        "            print(audio_id, true_label)\n",
        "        # Remove the \".wav\" extension\n",
        "        audio_id_without_extension = os.path.splitext(audio_id)[0]\n",
        "        output_data.append([audio_id_without_extension, pred])\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = correct_predictions / total_predictions\n",
        "\n",
        "# Print the results\n",
        "print(f\"Total predictions: {total_predictions}\")\n",
        "print(f\"Correct predictions: {correct_predictions}\")\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Write the output data to a file\n",
        "output_file_path = \"/content/output.tsv\"\n",
        "output_df = pd.DataFrame(output_data, columns=['audio_id', 'pred'])\n",
        "output_df.to_csv(output_file_path, sep='\\t', index=False, header=False)\n",
        "\n",
        "print(f\"Output file saved to: {output_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "342565c4-2523-47d5-ee13-6f6af525f4e5",
        "id": "ImRpwIRsNe2s"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5536decc9056cd290e16bc3773c24f98.wav 1\n",
            "741e22b830ff92f7c0af06605ab8dfed.wav 1\n",
            "9d8d8f1c55253f8b756c6cfe6880a90d.wav 0\n",
            "a235871885a6d84b14d64e339780b00d.wav 0\n",
            "d8ea8257efbf192f7fd270ec2ff73692.wav 1\n",
            "21509c8dd2f987bdce3863d1de090c80.wav 1\n",
            "4c12626db2221dc2ddd7d47caa0dd362.wav 0\n",
            "b70e9b26685b84191770825d2864b3be.wav 0\n",
            "02810575edce233e5cf84094e9064d17.wav 1\n",
            "f9c18bbb063b97077d913c3644d2b4f5.wav 0\n",
            "f06dad3ad84a40077faedfb8bb86b035.wav 0\n",
            "48083b0583c332a444d102efad1d1518.wav 1\n",
            "ce82fece3130d24a2717e8832fd36f25.wav 0\n",
            "85d0352307efd1eaea120c562c869e06.wav 1\n",
            "Total predictions: 1000\n",
            "Correct predictions: 986\n",
            "Accuracy: 0.99\n",
            "Output file saved to: /content/output.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## for test"
      ],
      "metadata": {
        "id": "CSMIaaMP-xpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the directory containing the audio files\n",
        "model_name_or_path = \"/content\"\n",
        "directory = '/content/test/test'\n",
        "audio_paths = get_audio_file_paths(directory) # Must be a list with absolute paths of the audios that will be used in inference\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "label2id = {\n",
        "    \"female\": 1,\n",
        "    \"male\": 0\n",
        "}\n",
        "\n",
        "id2label = {\n",
        "    1: \"female\",\n",
        "    0: \"male\"\n",
        "}\n",
        "\n",
        "num_labels = 2"
      ],
      "metadata": {
        "id": "jndT1Ugz-yuo"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_labels = 2\n",
        "\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name_or_path)\n",
        "model = AutoModelForAudioClassification.from_pretrained(\n",
        "    pretrained_model_name_or_path=model_name_or_path,\n",
        "    use_safetensors=True,\n",
        "    num_labels=num_labels,\n",
        "    label2id=label2id,\n",
        "    id2label=id2label,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lChE5r4z6tH8",
        "outputId": "5822b331-2acc-4f21-9294-1b9e31ca9b93"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at /content were not used when initializing Wav2Vec2ForSequenceClassification: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at /content and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = CustomDataset(audio_paths, max_audio_len=5)  # for 5-second audio\n",
        "\n",
        "data_collator = CollateFunc(\n",
        "    processor=feature_extractor,\n",
        "    padding=True,\n",
        "    sampling_rate=16000,\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=1,\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")"
      ],
      "metadata": {
        "id": "lDVDEPjI-yuo"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds, uncertain_filenames = predict(test_dataloader=test_dataloader, model=model, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHVS9sB1diun",
        "outputId": "ce81c023-9f95-426b-f8a6-f2cac381306f"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|▏         | 3/214 [00:09<08:46,  2.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7999496459960938 d5a5ab87926092c416645d33bee193a5.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 14/214 [00:20<03:21,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6254121661186218 c2b319ab0bf9704971154340b79b9ce3.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 22/214 [00:28<03:16,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9846464991569519 9b90b9bde03ef660d65af477e498e6ad.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 12%|█▏        | 26/214 [00:32<03:16,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.602676272392273 c87b9cc7b3f8fbde84ceb93c9ca97283.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▋        | 35/214 [00:42<03:03,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9759393930435181 1759c61f11986f671031e8d93126793a.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 36/214 [00:43<03:01,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9838926792144775 4e32567c043d6ff2c63744a6e22eb291.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|█▊        | 40/214 [00:47<02:55,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9150171279907227 76422187e39c778e71c655d289587f2e.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 23%|██▎       | 50/214 [00:57<02:41,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9855166077613831 f0aa30658a99d82ac7096d464a66406b.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 28%|██▊       | 59/214 [01:05<02:30,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9458574056625366 ec7ac8b411f14ecc398b915521b3e63e.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|██▉       | 64/214 [01:10<02:25,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9447989463806152 e7e10da633145756a79c39409fd16263.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 43%|████▎     | 93/214 [01:38<01:58,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9847645163536072 78b2f161520ea69b74e9061c0503ff00.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 94/214 [01:39<01:57,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9876993894577026 23eb3ab33bf2066d3f95d9922b6a77a7.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 51%|█████     | 109/214 [01:54<01:43,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8612250685691833 b04c4b0c90c3bbadbd4b6c4294edf312.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 57%|█████▋    | 121/214 [02:06<01:31,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6437684297561646 89587d0b96b84797fddf679a4341f197.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 64%|██████▎   | 136/214 [02:21<01:16,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.928317129611969 0ea9f37dbef6d9bc98c7afda4cffc51e.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 69%|██████▊   | 147/214 [02:31<01:05,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7563365697860718 87a9f759db5b1a8c8e6c259535ab2d1c.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|███████▍  | 158/214 [02:42<00:54,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9277634620666504 3745b62e313c79963ec0de66df55ab22.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 93%|█████████▎| 199/214 [03:22<00:14,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9884082674980164 16013e5be8c44f2240cfa4275af88bb3.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 213/214 [03:36<00:00,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9365003108978271 50230f9aab6da4fcf197a72e8e1573ea.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 214/214 [03:36<00:00,  1.01s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_data = []\n",
        "\n",
        "uncertain_filenames2 = ['d5a5ab87926092c416645d33bee193a5.wav', 'c2b319ab0bf9704971154340b79b9ce3.wav', 'c87b9cc7b3f8fbde84ceb93c9ca97283.wav']\n",
        "for audio_path, pred in zip(audio_paths, preds):\n",
        "    audio_id = os.path.basename(audio_path)\n",
        "    audio_id_without_extension = os.path.splitext(audio_id)[0]\n",
        "    if pred==0:\n",
        "        pred=1\n",
        "    else:\n",
        "        pred=0\n",
        "    output_data.append([audio_id_without_extension, pred])\n",
        "\n",
        "# Write the output data to a file\n",
        "output_file_path = \"/content/output23.tsv\"\n",
        "output_df = pd.DataFrame(output_data, columns=['audio_id', 'pred'])\n",
        "output_df.to_csv(output_file_path, sep='\\t', index=False, header=False)\n",
        "\n",
        "print(f\"Output file saved to: {output_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjGXtKang2tl",
        "outputId": "52fbf9a6-bca6-46b7-e3ef-de9f26910eb6"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output file saved to: /content/output23.tsv\n"
          ]
        }
      ]
    }
  ]
}