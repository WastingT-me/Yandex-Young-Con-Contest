{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Vc4zjy6ASw7U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "613e0bae-8d6b-40b2-db37-1043a66c5264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "zwAi_R0jwd_8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqFW7tR7Grsj"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define paths\n",
        "train_zip_path = '/content/drive/MyDrive/YoungCon/train.zip'\n",
        "test_zip_path = '/content/drive/MyDrive/YoungCon/test.zip'\n",
        "train_extract_path = '/content/train'\n",
        "test_extract_path = '/content/test'\n",
        "labels_folder = '/content/labels'\n",
        "\n",
        "# Unpack train.zip\n",
        "with zipfile.ZipFile(train_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(train_extract_path)\n",
        "\n",
        "# Unpack test.zip\n",
        "with zipfile.ZipFile(test_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(test_extract_path)\n",
        "\n",
        "# Move targets.tsv to labels folder\n",
        "if not os.path.exists(labels_folder):\n",
        "    os.makedirs(labels_folder)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.rename(os.path.join('/content/train/train/targets.tsv'), os.path.join(labels_folder, 'targets.tsv'))"
      ],
      "metadata": {
        "id": "XJPIoYcIKFQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def check_files_in_targets(train_folder, targets_file):\n",
        "    \"\"\"\n",
        "    Check if every file in the train folder is listed in the targets file.\n",
        "    Add \".wav\" extension to every record in the targets file.\n",
        "\n",
        "    Args:\n",
        "        train_folder (str): Path to the train folder containing audio files.\n",
        "        targets_file (str): Path to the targets.tsv file.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if all files in the train folder are listed in the targets file, False otherwise.\n",
        "        list: List of files in the train folder not listed in the targets file.\n",
        "    \"\"\"\n",
        "    # Read the targets file\n",
        "    targets_df = pd.read_csv(targets_file, sep='\\t')\n",
        "\n",
        "    # Add \".wav\" extension to each record in the targets file\n",
        "    targets_df.iloc[:, 0] = targets_df.iloc[:, 0].apply(lambda x: f\"{x}.wav\")\n",
        "\n",
        "    # Get the list of files in the train folder\n",
        "    train_files = [f for f in os.listdir(train_folder) if os.path.isfile(os.path.join(train_folder, f))]\n",
        "\n",
        "    # Get the list of files from the targets file\n",
        "    target_files = targets_df.iloc[:, 0].tolist()\n",
        "\n",
        "    # Check for files in the train folder not in the targets file\n",
        "    missing_files = [f for f in train_files if f not in target_files]\n",
        "\n",
        "    # Return the result\n",
        "    if missing_files:\n",
        "        return False, missing_files\n",
        "    else:\n",
        "        return True, []\n",
        "\n",
        "# Example usage\n",
        "train_folder = '/content/train/train'\n",
        "targets_file = '/content/labels/targets.tsv'\n",
        "\n",
        "all_files_present, missing_files = check_files_in_targets(train_folder, targets_file)\n",
        "if all_files_present:\n",
        "    print(\"All files in the train folder are listed in the targets file.\")\n",
        "else:\n",
        "    print(\"The following files in the train folder are not listed in the targets file:\")\n",
        "    for file in missing_files:\n",
        "        print(file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4F_eaEM-MRnL",
        "outputId": "2f304538-c2be-4e80-df78-b64d729c589a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following files in the train folder are not listed in the targets file:\n",
            "5d1f7e43366513a1d0a6ec5640c3dc24.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio\n",
        "import torch\n",
        "\n",
        "# Function to delete a specific file\n",
        "def delete_file(file_path):\n",
        "    if os.path.exists(file_path):\n",
        "        os.remove(file_path)\n",
        "        print(f\"Deleted file: {file_path}\")\n",
        "    else:\n",
        "        print(f\"File {file_path} not found.\")\n",
        "\n",
        "# Delete the specified file from the train folder\n",
        "train_audio_dir = '/content/train/train'\n",
        "file_to_delete = '5d1f7e43366513a1d0a6ec5640c3dc24.wav'\n",
        "delete_file(os.path.join(train_audio_dir, file_to_delete))\n",
        "\n",
        "# Load and modify targets\n",
        "targets_path = '/content/labels/targets.tsv'\n",
        "labels_df = pd.read_csv(targets_path, sep='\\t')\n",
        "labels_df.iloc[:, 0] = labels_df.iloc[:, 0].apply(lambda x: f\"{x}.wav\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dlPNea4NZDy",
        "outputId": "c62a6969-2bfb-4df2-9924-6a1c8d27e92e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted file: /content/train/train/5d1f7e43366513a1d0a6ec5640c3dc24.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pretrained model validation"
      ],
      "metadata": {
        "id": "iin8oa42tgmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from glob import glob\n",
        "from typing import List, Optional, Union, Dict\n",
        "\n",
        "import tqdm\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import functional as F\n",
        "from transformers import (\n",
        "    AutoFeatureExtractor,\n",
        "    AutoModelForAudioClassification,\n",
        "    Wav2Vec2Processor\n",
        ")\n"
      ],
      "metadata": {
        "id": "-FJWDbn6whiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset: List,\n",
        "        basedir: Optional[str] = None,\n",
        "        sampling_rate: int = 16000,\n",
        "        max_audio_len: int = 5,\n",
        "    ):\n",
        "        self.dataset = dataset\n",
        "        self.basedir = basedir\n",
        "\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.max_audio_len = max_audio_len\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the length of the dataset\n",
        "        \"\"\"\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.basedir is None:\n",
        "            filepath = self.dataset[index]\n",
        "        else:\n",
        "            filepath = os.path.join(self.basedir, self.dataset[index])\n",
        "\n",
        "        speech_array, sr = torchaudio.load(filepath)\n",
        "\n",
        "        if speech_array.shape[0] > 1:\n",
        "            speech_array = torch.mean(speech_array, dim=0, keepdim=True)\n",
        "\n",
        "        if sr != self.sampling_rate:\n",
        "            transform = torchaudio.transforms.Resample(sr, self.sampling_rate)\n",
        "            speech_array = transform(speech_array)\n",
        "            sr = self.sampling_rate\n",
        "\n",
        "        len_audio = speech_array.shape[1]\n",
        "\n",
        "        # Pad or truncate the audio to match the desired length\n",
        "        if len_audio < self.max_audio_len * self.sampling_rate:\n",
        "            # Pad the audio if it's shorter than the desired length\n",
        "            padding = torch.zeros(1, self.max_audio_len * self.sampling_rate - len_audio)\n",
        "            speech_array = torch.cat([speech_array, padding], dim=1)\n",
        "        else:\n",
        "            # Truncate the audio if it's longer than the desired length\n",
        "            speech_array = speech_array[:, :self.max_audio_len * self.sampling_rate]\n",
        "\n",
        "        speech_array = speech_array.squeeze().numpy()\n",
        "\n",
        "        return {\"input_values\": speech_array, \"attention_mask\": None}\n",
        "\n",
        "\n",
        "class CollateFunc:\n",
        "    def __init__(\n",
        "        self,\n",
        "        processor: Wav2Vec2Processor,\n",
        "        padding: Union[bool, str] = True,\n",
        "        pad_to_multiple_of: Optional[int] = None,\n",
        "        return_attention_mask: bool = True,\n",
        "        sampling_rate: int = 16000,\n",
        "        max_length: Optional[int] = None,\n",
        "    ):\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.processor = processor\n",
        "        self.padding = padding\n",
        "        self.pad_to_multiple_of = pad_to_multiple_of\n",
        "        self.return_attention_mask = return_attention_mask\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __call__(self, batch: List[Dict[str, np.ndarray]]):\n",
        "        # Extract input_values from the batch\n",
        "        input_values = [item[\"input_values\"] for item in batch]\n",
        "\n",
        "        batch = self.processor(\n",
        "            input_values,\n",
        "            sampling_rate=self.sampling_rate,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_attention_mask=self.return_attention_mask\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_values\": batch.input_values,\n",
        "            \"attention_mask\": batch.attention_mask if self.return_attention_mask else None\n",
        "        }"
      ],
      "metadata": {
        "id": "UZE4EeSHwfc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(test_dataloader, model, device: torch.device):\n",
        "    \"\"\"\n",
        "    Predict the class of the audio\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm.tqdm(test_dataloader):\n",
        "            input_values, attention_mask = batch['input_values'].to(device), batch['attention_mask'].to(device)\n",
        "\n",
        "            logits = model(input_values, attention_mask=attention_mask).logits\n",
        "            scores = F.softmax(logits, dim=-1)\n",
        "\n",
        "            pred = torch.argmax(scores, dim=1).cpu().detach().numpy()\n",
        "\n",
        "            preds.extend(pred)\n",
        "\n",
        "    return preds\n",
        "\n",
        "\n",
        "def get_gender(model_name_or_path: str, audio_paths: List[str], label2id: Dict, id2label: Dict, device: torch.device):\n",
        "    num_labels = 2\n",
        "\n",
        "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_name_or_path)\n",
        "    model = AutoModelForAudioClassification.from_pretrained(\n",
        "        pretrained_model_name_or_path=model_name_or_path,\n",
        "        use_safetensors=True,\n",
        "        num_labels=num_labels,\n",
        "        label2id=label2id,\n",
        "        id2label=id2label,\n",
        "    )\n",
        "\n",
        "    test_dataset = CustomDataset(audio_paths, max_audio_len=5)  # for 5-second audio\n",
        "\n",
        "    data_collator = CollateFunc(\n",
        "        processor=feature_extractor,\n",
        "        padding=True,\n",
        "        sampling_rate=16000,\n",
        "    )\n",
        "\n",
        "    test_dataloader = DataLoader(\n",
        "        dataset=test_dataset,\n",
        "        batch_size=16,\n",
        "        collate_fn=data_collator,\n",
        "        shuffle=False,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    preds = predict(test_dataloader=test_dataloader, model=model, device=device)\n",
        "\n",
        "    return preds\n",
        "\n",
        "def get_audio_file_paths(directory, extensions=['.wav', '.mp3', '.flac']):\n",
        "    audio_files = []\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if any(file.lower().endswith(ext) for ext in extensions):\n",
        "                audio_files.append(os.path.abspath(os.path.join(root, file)))\n",
        "    return audio_files"
      ],
      "metadata": {
        "id": "BuTGbCWq2Bam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the directory containing the audio files\n",
        "model_name_or_path = \"/content\"\n",
        "directory = '/content/train/train'\n",
        "audio_paths = get_audio_file_paths(directory) # Must be a list with absolute paths of the audios that will be used in inference\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "label2id = {\n",
        "    \"female\": 1,\n",
        "    \"male\": 0\n",
        "}\n",
        "\n",
        "id2label = {\n",
        "    1: \"female\",\n",
        "    0: \"male\"\n",
        "}\n",
        "\n",
        "num_labels = 2"
      ],
      "metadata": {
        "id": "Gnjmbchu2IFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_labels = 2\n",
        "\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name_or_path)\n",
        "model = AutoModelForAudioClassification.from_pretrained(\n",
        "    pretrained_model_name_or_path=model_name_or_path,\n",
        "    use_safetensors=True,\n",
        "    num_labels=num_labels,\n",
        "    label2id=label2id,\n",
        "    id2label=id2label,\n",
        ")"
      ],
      "metadata": {
        "id": "BJCnK0Ydtnqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = CustomDataset(audio_paths, max_audio_len=5)  # for 5-second audio\n",
        "\n",
        "data_collator = CollateFunc(\n",
        "    processor=feature_extractor,\n",
        "    padding=True,\n",
        "    sampling_rate=16000,\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=128,\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")"
      ],
      "metadata": {
        "id": "1N0lWHPq2pnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = predict(test_dataloader=test_dataloader, model=model, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L47i0cOl2LJf",
        "outputId": "8fa0e481-6801-40ef-a0c3-bc69dae4cf39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 871/871 [13:45<00:00,  1.05it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.savetxt(\"preds_train.npy\", np.array(preds))"
      ],
      "metadata": {
        "id": "cnqt1nb55_f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "targets_file_path = \"/content/labels/targets.tsv\"\n",
        "targets_df = pd.read_csv(targets_file_path, sep='\\t', header=None, names=['audio_id', 'true_label'])\n",
        "targets_df['audio_id'] = targets_df['audio_id'] + '.wav'\n",
        "true_labels_dict = dict(zip(targets_df['audio_id'], targets_df['true_label']))\n",
        "\n",
        "\n",
        "correct_predictions = 0\n",
        "total_predictions = len(audio_paths)\n",
        "output_data = []\n",
        "\n",
        "for audio_path, pred in zip(audio_paths, preds):\n",
        "    audio_id = os.path.basename(audio_path)\n",
        "    true_label = true_labels_dict.get(audio_id, None)\n",
        "    if pred==0:\n",
        "        pred=1\n",
        "    else:\n",
        "        pred=0\n",
        "    if true_label is not None:\n",
        "        if pred == true_label:\n",
        "            correct_predictions += 1\n",
        "        # Remove the \".wav\" extension\n",
        "        audio_id_without_extension = os.path.splitext(audio_id)[0]\n",
        "        output_data.append([audio_id_without_extension, pred])\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = correct_predictions / total_predictions\n",
        "print(f\"Total predictions: {total_predictions}\")\n",
        "print(f\"Correct predictions: {correct_predictions}\")\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "output_file_path = \"/content/output.tsv\"\n",
        "output_df = pd.DataFrame(output_data, columns=['audio_id', 'pred'])\n",
        "output_df.to_csv(output_file_path, sep='\\t', index=False, header=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ditNfVDC5V8M",
        "outputId": "1edc619e-4db7-4728-c6e5-ad0be1936cc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total predictions: 13935\n",
            "Correct predictions: 13604\n",
            "Accuracy: 0.98\n",
            "Output file saved to: /content/output.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## for test"
      ],
      "metadata": {
        "id": "CSMIaaMP-xpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the directory containing the audio files\n",
        "model_name_or_path = \"/content\"\n",
        "directory = '/content/test/test'\n",
        "audio_paths = get_audio_file_paths(directory) # Must be a list with absolute paths of the audios that will be used in inference\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "label2id = {\n",
        "    \"female\": 1,\n",
        "    \"male\": 0\n",
        "}\n",
        "\n",
        "id2label = {\n",
        "    1: \"female\",\n",
        "    0: \"male\"\n",
        "}\n",
        "\n",
        "num_labels = 2"
      ],
      "metadata": {
        "id": "jndT1Ugz-yuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_labels = 2\n",
        "\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name_or_path)\n",
        "model = AutoModelForAudioClassification.from_pretrained(\n",
        "    pretrained_model_name_or_path=model_name_or_path,\n",
        "    use_safetensors=True,\n",
        "    num_labels=num_labels,\n",
        "    label2id=label2id,\n",
        "    id2label=id2label,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lChE5r4z6tH8",
        "outputId": "748d53ab-4d81-4cff-939d-43072bc9dc7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at /content were not used when initializing Wav2Vec2ForSequenceClassification: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at /content and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = CustomDataset(audio_paths, max_audio_len=5)  # for 5-second audio\n",
        "\n",
        "data_collator = CollateFunc(\n",
        "    processor=feature_extractor,\n",
        "    padding=True,\n",
        "    sampling_rate=16000,\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")"
      ],
      "metadata": {
        "id": "lDVDEPjI-yuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = predict(test_dataloader=test_dataloader, model=model, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dca3fa8-b3e8-4126-e627-d41efe31181f",
        "id": "A2NrobvE-yup"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 214/214 [03:30<00:00,  1.02it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.savetxt(\"preds_test.npy\", np.array(preds))"
      ],
      "metadata": {
        "id": "pHy1Q3b2-yup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "output_data = []\n",
        "\n",
        "# Compare predictions with true labels and prepare the output data\n",
        "for audio_path, pred in zip(audio_paths, preds):\n",
        "    audio_id = os.path.basename(audio_path)\n",
        "    if pred==0:\n",
        "        pred=1\n",
        "    else:\n",
        "        pred=0\n",
        "    audio_id_without_extension = os.path.splitext(audio_id)[0]\n",
        "    output_data.append([audio_id_without_extension, pred])\n",
        "\n",
        "output_file_path = \"/content/output4.tsv\"\n",
        "output_df = pd.DataFrame(output_data, columns=['audio_id', 'pred'])\n",
        "output_df.to_csv(output_file_path, sep='\\t', index=False, header=False)\n",
        "\n",
        "print(f\"Output file saved to: {output_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4be45b6-6c64-4eff-8f07-b9e2809bcbf4",
        "id": "VUniPJbG-yup"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output file saved to: /content/output4.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop"
      ],
      "metadata": {
        "id": "L-YAesTuPqTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from transformers import Wav2Vec2Processor, AutoModelForAudioClassification, AutoFeatureExtractor, get_scheduler\n",
        "from torch.optim import Adam\n",
        "from typing import List, Optional, Union, Dict\n",
        "import numpy as np\n",
        "import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataset: List, labels: List, basedir: Optional[str] = None, sampling_rate: int = 16000, max_audio_len: int = 5):\n",
        "        self.dataset = dataset\n",
        "        self.labels = labels\n",
        "        self.basedir = basedir\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.max_audio_len = max_audio_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.basedir is None:\n",
        "            filepath = self.dataset[index]\n",
        "        else:\n",
        "            filepath = os.path.join(self.basedir, self.dataset[index])\n",
        "\n",
        "        speech_array, sr = torchaudio.load(filepath)\n",
        "        if speech_array.shape[0] > 1:\n",
        "            speech_array = torch.mean(speech_array, dim=0, keepdim=True)\n",
        "\n",
        "        if sr != self.sampling_rate:\n",
        "            transform = torchaudio.transforms.Resample(sr, self.sampling_rate)\n",
        "            speech_array = transform(speech_array)\n",
        "            sr = self.sampling_rate\n",
        "\n",
        "        len_audio = speech_array.shape[1]\n",
        "        if len_audio < self.max_audio_len * self.sampling_rate:\n",
        "            padding = torch.zeros(1, self.max_audio_len * self.sampling_rate - len_audio)\n",
        "            speech_array = torch.cat([speech_array, padding], dim=1)\n",
        "        else:\n",
        "            speech_array = speech_array[:, :self.max_audio_len * self.sampling_rate]\n",
        "\n",
        "        speech_array = speech_array.squeeze().numpy()\n",
        "        label = self.labels[index]\n",
        "\n",
        "        return {\"input_values\": speech_array, \"attention_mask\": None, \"labels\": label}\n",
        "\n",
        "class CollateFunc:\n",
        "    def __init__(self, processor: Wav2Vec2Processor, padding: Union[bool, str] = True, pad_to_multiple_of: Optional[int] = None,\n",
        "                 return_attention_mask: bool = True, sampling_rate: int = 16000, max_length: Optional[int] = None):\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.processor = processor\n",
        "        self.padding = padding\n",
        "        self.pad_to_multiple_of = pad_to_multiple_of\n",
        "        self.return_attention_mask = return_attention_mask\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __call__(self, batch: List[Dict[str, np.ndarray]]):\n",
        "        input_values = [item[\"input_values\"] for item in batch]\n",
        "        labels = [item[\"labels\"] for item in batch]\n",
        "\n",
        "        batch = self.processor(\n",
        "            input_values,\n",
        "            sampling_rate=self.sampling_rate,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_attention_mask=self.return_attention_mask\n",
        "        )\n",
        "\n",
        "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.long)\n",
        "        return batch\n",
        "\n",
        "def train(train_dataloader, model, optimizer, lr_scheduler, device, gradient_accumulation_steps):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for batch_idx, batch in enumerate(tqdm.tqdm(train_dataloader)):\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        input_values = batch['input_values'].to(device, dtype=torch.float16)\n",
        "        attention_mask = batch['attention_mask'].to(device, dtype=torch.float16)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # Debugging statements\n",
        "        #print(f\"Batch {batch_idx} - Input values shape: {input_values.shape}, Attention mask shape: {attention_mask.shape}, Labels shape: {labels.shape}\")\n",
        "\n",
        "        with autocast():\n",
        "            outputs = model(input_values, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss / gradient_accumulation_steps\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()  # Ensure gradients are zeroed after the update\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = torch.argmax(outputs.logits, dim=-1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def evaluate(eval_dataloader, model, device):\n",
        "    model.eval()\n",
        "    total_eval_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm.tqdm(eval_dataloader):\n",
        "            input_values = batch['input_values'].to(device, dtype=torch.float16)\n",
        "            attention_mask = batch['attention_mask'].to(device, dtype=torch.float16)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Debugging statements\n",
        "            #print(f\"Eval - Input values shape: {input_values.shape}, Attention mask shape: {attention_mask.shape}, Labels shape: {labels.shape}\")\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(input_values, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "\n",
        "            total_eval_loss += loss.item()\n",
        "            preds = torch.argmax(outputs.logits, dim=-1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_eval_loss = total_eval_loss / len(eval_dataloader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    return avg_eval_loss, accuracy\n",
        "\n",
        "# Function to plot training and validation loss and accuracy\n",
        "def plot_metrics(train_losses, val_losses, train_accuracies, val_accuracies):\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "    plt.figure(figsize=(14, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_losses, 'b', label='Training Loss')\n",
        "    plt.plot(epochs, val_losses, 'r', label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, train_accuracies, 'b', label='Training Accuracy')\n",
        "    plt.plot(epochs, val_accuracies, 'r', label='Validation Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Initialize model, processor, dataloaders, etc.\n",
        "model_name_or_path = \"/content\"\n",
        "directory = '/content/train/train'\n",
        "audio_paths = [os.path.join(directory, fname) for fname in os.listdir(directory) if fname.endswith('.wav')]\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Path to the targets.tsv file\n",
        "targets_file_path = \"/content/labels/targets.tsv\"\n",
        "\n",
        "# Load the targets.tsv file into a pandas DataFrame\n",
        "targets_df = pd.read_csv(targets_file_path, sep='\\t', header=None, names=['audio_id', 'true_label'])\n",
        "\n",
        "# Append \".wav\" to audio_id to match the audio file paths\n",
        "targets_df['audio_id'] = targets_df['audio_id'] + '.wav'\n",
        "\n",
        "# Create a dictionary from audio_id to true_label\n",
        "labels = targets_df['true_label'].map({0: 1, 1: 0}).tolist()\n",
        "\n",
        "label2id = {\"female\": 1, \"male\": 0}\n",
        "id2label = {1: \"female\", 0: \"male\"}\n",
        "\n",
        "num_labels = len(label2id)\n",
        "\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name_or_path)\n",
        "model = AutoModelForAudioClassification.from_pretrained(\n",
        "    pretrained_model_name_or_path=model_name_or_path,\n",
        "    use_safetensors=True,\n",
        "    num_labels=num_labels,\n",
        "    label2id=label2id,\n",
        "    id2label=id2label,\n",
        ").to(device)\n",
        "\n",
        "# Split the dataset into training and evaluation\n",
        "dataset = CustomDataset(audio_paths[:1000], labels, max_audio_len=5)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "eval_size = len(dataset) - train_size\n",
        "train_dataset, eval_dataset = random_split(dataset, [train_size, eval_size])\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=2, collate_fn=CollateFunc(processor=feature_extractor, padding=True, sampling_rate=16000), shuffle=True, num_workers=2)\n",
        "eval_dataloader = DataLoader(dataset=eval_dataset, batch_size=2, collate_fn=CollateFunc(processor=feature_extractor, padding=True, sampling_rate=16000), shuffle=False, num_workers=2)\n",
        "\n",
        "##############################################\n",
        "\n",
        "class CustomDataset2(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset: List,\n",
        "        basedir: Optional[str] = None,\n",
        "        sampling_rate: int = 16000,\n",
        "        max_audio_len: int = 5,\n",
        "    ):\n",
        "        self.dataset = dataset\n",
        "        self.basedir = basedir\n",
        "\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.max_audio_len = max_audio_len\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the length of the dataset\n",
        "        \"\"\"\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.basedir is None:\n",
        "            filepath = self.dataset[index]\n",
        "        else:\n",
        "            filepath = os.path.join(self.basedir, self.dataset[index])\n",
        "\n",
        "        speech_array, sr = torchaudio.load(filepath)\n",
        "\n",
        "        if speech_array.shape[0] > 1:\n",
        "            speech_array = torch.mean(speech_array, dim=0, keepdim=True)\n",
        "\n",
        "        if sr != self.sampling_rate:\n",
        "            transform = torchaudio.transforms.Resample(sr, self.sampling_rate)\n",
        "            speech_array = transform(speech_array)\n",
        "            sr = self.sampling_rate\n",
        "\n",
        "        len_audio = speech_array.shape[1]\n",
        "\n",
        "        # Pad or truncate the audio to match the desired length\n",
        "        if len_audio < self.max_audio_len * self.sampling_rate:\n",
        "            # Pad the audio if it's shorter than the desired length\n",
        "            padding = torch.zeros(1, self.max_audio_len * self.sampling_rate - len_audio)\n",
        "            speech_array = torch.cat([speech_array, padding], dim=1)\n",
        "        else:\n",
        "            # Truncate the audio if it's longer than the desired length\n",
        "            speech_array = speech_array[:, :self.max_audio_len * self.sampling_rate]\n",
        "\n",
        "        speech_array = speech_array.squeeze().numpy()\n",
        "\n",
        "        return {\"input_values\": speech_array, \"attention_mask\": None}\n",
        "\n",
        "def predict(test_dataloader, model, device: torch.device):\n",
        "    \"\"\"\n",
        "    Predict the class of the audio\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm.tqdm(test_dataloader):\n",
        "            input_values, attention_mask = batch['input_values'].to(device), batch['attention_mask'].to(device)\n",
        "\n",
        "            logits = model(input_values, attention_mask=attention_mask).logits\n",
        "            scores = F.softmax(logits, dim=-1)\n",
        "\n",
        "            pred = torch.argmax(scores, dim=1).cpu().detach().numpy()\n",
        "\n",
        "            preds.extend(pred)\n",
        "\n",
        "    return preds\n",
        "\n",
        "def get_audio_file_paths(directory, extensions=['.wav', '.mp3', '.flac']):\n",
        "    audio_files = []\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if any(file.lower().endswith(ext) for ext in extensions):\n",
        "                audio_files.append(os.path.abspath(os.path.join(root, file)))\n",
        "    return audio_files\n",
        "\n",
        "audio_paths_test = get_audio_file_paths('/content/test/test')\n",
        "test_dataset = CustomDataset2(audio_paths_test, max_audio_len=5)\n",
        "\n",
        "data_collator = CollateFunc(\n",
        "    processor=feature_extractor,\n",
        "    padding=True,\n",
        "    sampling_rate=16000,\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")\n",
        "#######################################################\n",
        "\n",
        "# Optimizer and scheduler\n",
        "optimizer = Adam(model.parameters(), lr=5e-5)\n",
        "num_training_steps = len(train_dataloader) * 10  # Assuming 10 epochs\n",
        "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "gradient_accumulation_steps = 1\n",
        "train_losses, val_losses = [], []\n",
        "train_accuracies, val_accuracies = [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train(train_dataloader, model, optimizer, lr_scheduler, device, gradient_accumulation_steps)\n",
        "    val_loss, val_acc = evaluate(eval_dataloader, model, device)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    preds = predict(test_dataloader=test_dataloader, model=model, device=device)\n",
        "    output_data = []\n",
        "    # Compare predictions with true labels and prepare the output data\n",
        "    for audio_path, pred in zip(audio_paths_test, preds):\n",
        "        audio_id = os.path.basename(audio_path)  # Get the audio_id from the file path\n",
        "        if pred==0:\n",
        "            pred=1\n",
        "        else:\n",
        "            pred=0\n",
        "        audio_id_without_extension = os.path.splitext(audio_id)[0]\n",
        "        output_data.append([audio_id_without_extension, pred])\n",
        "\n",
        "    output_df = pd.DataFrame(output_data, columns=['audio_id', 'pred'])\n",
        "    output_df.to_csv(f\"/content/answer{epoch}.tsv\", sep='\\t', index=False, header=False)\n",
        "\n",
        "    model.save_pretrained(f\"/content/epoch_{epoch}.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Wtha4_fEN6FP",
        "outputId": "b458b6fb-523f-4c12-94da-c270b7f3a284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at /content were not used when initializing Wav2Vec2ForSequenceClassification: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at /content and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|██████████| 400/400 [02:04<00:00,  3.22it/s]\n",
            "100%|██████████| 100/100 [00:11<00:00,  8.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Train Loss: 0.7743, Train Accuracy: 0.5088\n",
            "Val Loss: 0.6953, Val Accuracy: 0.5150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/214 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"<ipython-input-4-a8c647be100f>\", line 70, in __call__\n    labels = [item[\"labels\"] for item in batch]\n  File \"<ipython-input-4-a8c647be100f>\", line 70, in <listcomp>\n    labels = [item[\"labels\"] for item in batch]\nKeyError: 'labels'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a8c647be100f>\u001b[0m in \u001b[0;36m<cell line: 326>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m     \u001b[0moutput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;31m# Compare predictions with true labels and prepare the output data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-a8c647be100f>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(test_dataloader, model, device)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0minput_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_values'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1346\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"<ipython-input-4-a8c647be100f>\", line 70, in __call__\n    labels = [item[\"labels\"] for item in batch]\n  File \"<ipython-input-4-a8c647be100f>\", line 70, in <listcomp>\n    labels = [item[\"labels\"] for item in batch]\nKeyError: 'labels'\n"
          ]
        }
      ]
    }
  ]
}